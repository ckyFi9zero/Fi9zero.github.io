<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>故事的开始</title>
      <link href="/2022/12/27/%E6%95%85%E4%BA%8B%E7%9A%84%E5%BC%80%E5%A7%8B/"/>
      <url>/2022/12/27/%E6%95%85%E4%BA%8B%E7%9A%84%E5%BC%80%E5%A7%8B/</url>
      
        <content type="html"><![CDATA[<h1 id="故事开始啦"><a href="#故事开始啦" class="headerlink" title="故事开始啦"></a>故事开始啦</h1><p>从建站到购买域名再到现在正式开放应该差不多两周时间吧，其中参考了很多大佬的博客。<br>非常感谢以下大佬！！！</p><h2 id="fomalhaut"><a href="#fomalhaut" class="headerlink" title="fomalhaut"></a><a href="https://fomal.cc/">fomalhaut</a></h2><p><img src="https://cdn.staticaly.com/gh/ckyFi9zero/pic_bed1@main/pic_bed/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20221227172317.1fjgzx2w7ccg.webp" alt="fomalhuat"></p><h2 id="唐志远の博客"><a href="#唐志远の博客" class="headerlink" title="唐志远の博客"></a><a href="https://tzy1997.com/">唐志远の博客</a></h2><p><img src="https://cdn.staticaly.com/gh/ckyFi9zero/pic_bed1@main/pic_bed/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20221227172254.3y3pp36hcfg0.webp" alt="tzy"></p><h2 id="咕咕鸽"><a href="#咕咕鸽" class="headerlink" title="咕咕鸽"></a><a href="https://guguge.top/">咕咕鸽</a></h2><p><img src="https://cdn.staticaly.com/gh/ckyFi9zero/pic_bed1@main/pic_bed/%E5%BE%AE%E4%BF%A1%E6%88%AA%E5%9B%BE_20221227172222.5z3mb0k9r000.webp" alt="guguge"></p><h1 id="建站后话"><a href="#建站后话" class="headerlink" title="建站后话"></a>建站后话</h1><p>接下来本站会不定期更新，关于学习笔记或是吐槽生活。<br>此外，本站目前主要功能还是以撰写文章为主，评论等等其他功能会慢慢开放。<br>致敬我的第一篇博客啊哈哈哈哈哈哈!</p>]]></content>
      
      
      <categories>
          
          <category> 聊聊生活 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 生活 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>多层感知机(MLP)</title>
      <link href="/2022/12/27/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%EF%BC%88MLP%EF%BC%89/"/>
      <url>/2022/12/27/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%EF%BC%88MLP%EF%BC%89/</url>
      
        <content type="html"><![CDATA[<h1 id="多层感知机（MLP）"><a href="#多层感知机（MLP）" class="headerlink" title="多层感知机（MLP）"></a>多层感知机（MLP）</h1><p>[TOC]</p><h2 id="线性函数"><a href="#线性函数" class="headerlink" title="线性函数"></a>线性函数</h2><p>y&#x3D;kx+b<br>一元一次方程</p><h2 id="激活函数"><a href="#激活函数" class="headerlink" title="激活函数"></a>激活函数</h2><h3 id="1-为什么要使用激活函数"><a href="#1-为什么要使用激活函数" class="headerlink" title="1.为什么要使用激活函数"></a>1.为什么要使用激活函数</h3><p> a. 不使用激活函数，每一层输出都是上层输入的线性函数，无论神经网络有多少层，输出都是输入的线性组合。</p><p>b. 使用激活函数，能够给神经元引入非线性因素，使得神经网络可以任意逼近任何非线性函数，这样神经网络就可以利用到更多的非线性模型中。</p><h3 id="2-激活函数需要具备的性质"><a href="#2-激活函数需要具备的性质" class="headerlink" title="2. 激活函数需要具备的性质:"></a>2. 激活函数需要具备的性质:</h3><p>a.连续并可导（允许少数点上不可导）的非线性函数。可导的激活函数可以直接利用数值优化的方法来学习网络参数。</p><p>b.激活函数及其导函数要尽可能的简单，有利于提高网络计算效率。</p><p>c.激活函数的导函数的值域要在一个合适的区间内，不能太大也不能太小，否则会影响训练的效率和稳定性。 </p><h4 id="3-常用的激活函数"><a href="#3-常用的激活函数" class="headerlink" title="3.常用的激活函数"></a>3.常用的激活函数</h4><h4 id="ReLU"><a href="#ReLU" class="headerlink" title="ReLU"></a>ReLU</h4><p> <img src="/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%EF%BC%88MLP%EF%BC%89.assets/output_mlp_76f463_18_0.svg" alt="../_images/output_mlp_76f463_18_0.svg"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">y = torch.relu(x)</span><br></pre></td></tr></table></figure><h4 id="sigmoid"><a href="#sigmoid" class="headerlink" title="sigmoid"></a>sigmoid</h4><p><img src="/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%EF%BC%88MLP%EF%BC%89.assets/image-20221201151107183.png" alt="image-20221201151107183"></p><p><img src="/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%EF%BC%88MLP%EF%BC%89.assets/image-20221201151030747.png" alt="image-20221201151030747"></p><p><img src="/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%EF%BC%88MLP%EF%BC%89.assets/image-20221201151126977.png" alt="image-20221201151126977"></p><h4 id="tanh"><a href="#tanh" class="headerlink" title="tanh"></a>tanh</h4><p><img src="/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%EF%BC%88MLP%EF%BC%89.assets/image-20221201151220094.png" alt="image-20221201151220094"></p><p><img src="/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%EF%BC%88MLP%EF%BC%89.assets/image-20221201151229733.png" alt="image-20221201151229733"></p><p><img src="/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%EF%BC%88MLP%EF%BC%89.assets/image-20221201151240861.png" alt="image-20221201151240861"></p><h2 id="多层感知机"><a href="#多层感知机" class="headerlink" title="多层感知机"></a>多层感知机</h2><h3 id="隐藏层"><a href="#隐藏层" class="headerlink" title="隐藏层"></a>隐藏层</h3><p>我们可以通过在网络中加入一个或多个隐藏层来克服线性模型的限制， 使其能处理更普遍的函数关系类型。 要做到这一点，最简单的方法是将许多全连接层堆叠在一起。 每一层都输出到上面的层，直到生成最后的输出。 我们可以把前L−1层看作表示，把最后一层看作线性预测器。 这种架构通常称为<em>多层感知机</em>（multilayer perceptron），通常缩写为<em>MLP</em>。</p><img src="多层感知机（MLP）.assets/image-20221201151548366.png" alt="image-20221201151548366" style="zoom: 80%;" /><h3 id="单隐藏层多分类问题"><a href="#单隐藏层多分类问题" class="headerlink" title="单隐藏层多分类问题"></a>单隐藏层多分类问题</h3><p><img src="/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%EF%BC%88MLP%EF%BC%89.assets/image-20221201152625576.png" alt="image-20221201152625576"></p><h4 id="Softmax"><a href="#Softmax" class="headerlink" title="Softmax"></a>Softmax</h4><p><img src="/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%EF%BC%88MLP%EF%BC%89.assets/image-20221201152841005.png" alt="image-20221201152841005"></p><h3 id="多隐藏层"><a href="#多隐藏层" class="headerlink" title="多隐藏层"></a>多隐藏层</h3><img src="多层感知机（MLP）.assets/image-20221201153043507.png" alt="image-20221201153043507" style="zoom:50%;" /><img src="多层感知机（MLP）.assets/image-20221201153125004.png" alt="image-20221201153125004" style="zoom:50%;" /><h1 id="pytorch的使用"><a href="#pytorch的使用" class="headerlink" title="pytorch的使用"></a>pytorch的使用</h1><h2 id="线性模型"><a href="#线性模型" class="headerlink" title="线性模型"></a>线性模型</h2><h3 id="问题引入"><a href="#问题引入" class="headerlink" title="问题引入"></a>问题引入</h3><p>以知小明1小时打2行代码，2分钟4行，3分钟6行，问：4分钟打几行代码？</p><table><thead><tr><th>x</th><th></th><th>y</th></tr></thead><tbody><tr><td>1</td><td></td><td>2</td></tr><tr><td>2</td><td></td><td>4</td></tr><tr><td>3</td><td></td><td>6</td></tr><tr><td>4</td><td></td><td>?</td></tr></tbody></table><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment">#数据作为矩阵参与Tensor计算</span></span><br><span class="line">x_data = torch.Tensor([<span class="number">1.0</span>],[<span class="number">2.0</span>],[<span class="number">3.0</span>])</span><br><span class="line">y_data = torch.Tensor([<span class="number">2.0</span>],[<span class="number">4.0</span>],[<span class="number">6.0</span>])</span><br></pre></td></tr></table></figure><h3 id=""><a href="#" class="headerlink" title=""></a></h3><h4 id="模型设计"><a href="#模型设计" class="headerlink" title="模型设计"></a>模型设计</h4><p>设y &#x3D; w * x +b</p><p>pytorch代码</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#固定继承于Module</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearModel</span>(torch.nn.Module):</span><br><span class="line">    <span class="comment">#构造函数初始化</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment">#调用父类的init</span></span><br><span class="line">        <span class="built_in">super</span>(LinearModel, self).__init__()</span><br><span class="line">        <span class="comment">#Linear对象包括weight(w)以及bias(b)两个成员张量</span></span><br><span class="line">        self.linear = torch.nn.Linear(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#前馈函数forward，对父类函数中的overwrite</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment">#调用linear中的call()，以利用父类forward()计算wx+b</span></span><br><span class="line">        y_pred = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line">    <span class="comment">#反馈函数backward由module自动根据计算图生成</span></span><br></pre></td></tr></table></figure><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p><img src="/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%EF%BC%88MLP%EF%BC%89.assets/image-20221201155458250.png" alt="image-20221201155458250"></p><p><img src="/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%EF%BC%88MLP%EF%BC%89.assets/image-20221201162145490.png" alt="image-20221201162145490"></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">criterion = torch.nn.MSELoss(size_average=<span class="literal">False</span>)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"></span><br></pre></td></tr></table></figure><p><img src="/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%EF%BC%88MLP%EF%BC%89.assets/image-20210301143950959.png" alt="w=3"></p><p><img src="/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%EF%BC%88MLP%EF%BC%89.assets/image-20210301144058452.png" alt="w=4"></p><p><img src="/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%EF%BC%88MLP%EF%BC%89.assets/image-20210301144201509.png" alt="w=2"></p><p><img src="/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%EF%BC%88MLP%EF%BC%89.assets/image-20210301145240283.png" alt="不同w的MSE"></p><p><img src="/%E5%A4%9A%E5%B1%82%E6%84%9F%E7%9F%A5%E6%9C%BA%EF%BC%88MLP%EF%BC%89.assets/image-20210301151326189.png" alt="随着w的变化所绘制的loss曲线图"></p><h4 id="梯度下降"><a href="#梯度下降" class="headerlink" title="梯度下降"></a>梯度下降</h4><p>$$<br>\omega &#x3D; \omega - \alpha \frac{\partial cost}{\partial \omega}<br>$$</p><h4 id="优化器"><a href="#优化器" class="headerlink" title="优化器"></a>优化器</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">#model.parameters()用于检查模型中所能进行优化的张量</span></span><br><span class="line"><span class="comment">#learningrate(lr)表学习率，可以统一也可以不统一</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br></pre></td></tr></table></figure><h4 id="训练过程"><a href="#训练过程" class="headerlink" title="训练过程"></a>训练过程</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">100</span>):</span><br><span class="line">    <span class="comment">#前馈计算y_pred</span></span><br><span class="line">    y_pred = model(x_data)</span><br><span class="line">    <span class="comment">#前馈计算损失loss</span></span><br><span class="line">    loss = criterion(y_pred,y_data)</span><br><span class="line">    <span class="comment">#打印调用loss时，会自动调用内部__str__()函数，避免产生计算图</span></span><br><span class="line">    <span class="built_in">print</span>(epoch,loss)</span><br><span class="line">    <span class="comment">#梯度清零</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="comment">#梯度反向传播，计算图清除</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment">#根据传播的梯度以及学习率更新参数</span></span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure><h4 id="整体代码"><a href="#整体代码" class="headerlink" title="整体代码"></a>整体代码</h4><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="comment">#数据作为矩阵参与Tensor计算</span></span><br><span class="line">x_data = torch.Tensor([[<span class="number">1.0</span>],[<span class="number">2.0</span>],[<span class="number">3.0</span>]])</span><br><span class="line">y_data = torch.Tensor([[<span class="number">2.0</span>],[<span class="number">4.0</span>],[<span class="number">6.0</span>]])</span><br><span class="line"></span><br><span class="line"><span class="comment">#固定继承于Module</span></span><br><span class="line"><span class="keyword">class</span> <span class="title class_">LinearModel</span>(torch.nn.Module):</span><br><span class="line">    <span class="comment">#构造函数初始化</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">__init__</span>(<span class="params">self</span>):</span><br><span class="line">        <span class="comment">#调用父类的init</span></span><br><span class="line">        <span class="built_in">super</span>(LinearModel, self).__init__()</span><br><span class="line">        <span class="comment">#Linear对象包括weight(w)以及bias(b)两个成员张量</span></span><br><span class="line">        self.linear = torch.nn.Linear(<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">    <span class="comment">#前馈函数forward，对父类函数中的overwrite</span></span><br><span class="line">    <span class="keyword">def</span> <span class="title function_">forward</span>(<span class="params">self, x</span>):</span><br><span class="line">        <span class="comment">#调用linear中的call()，以利用父类forward()计算wx+b</span></span><br><span class="line">        y_pred = self.linear(x)</span><br><span class="line">        <span class="keyword">return</span> y_pred</span><br><span class="line">    <span class="comment">#反馈函数backward由module自动根据计算图生成</span></span><br><span class="line">model = LinearModel()</span><br><span class="line"></span><br><span class="line"><span class="comment">#构造的criterion对象所接受的参数为（y&#x27;,y）</span></span><br><span class="line">criterion = torch.nn.MSELoss(size_average=<span class="literal">False</span>)</span><br><span class="line"><span class="comment">#model.parameters()用于检查模型中所能进行优化的张量</span></span><br><span class="line"><span class="comment">#learningrate(lr)表学习率，可以统一也可以不统一</span></span><br><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=<span class="number">0.01</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> <span class="built_in">range</span>(<span class="number">1000</span>):</span><br><span class="line">    <span class="comment">#前馈计算y_pred</span></span><br><span class="line">    y_pred = model(x_data)</span><br><span class="line">    <span class="comment">#前馈计算损失loss</span></span><br><span class="line">    loss = criterion(y_pred,y_data)</span><br><span class="line">    <span class="comment">#打印调用loss时，会自动调用内部__str__()函数，避免产生计算图</span></span><br><span class="line">    <span class="built_in">print</span>(epoch,loss)</span><br><span class="line">    <span class="comment">#梯度清零</span></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    <span class="comment">#梯度反向传播，计算图清除</span></span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment">#根据传播的梯度以及学习率更新参数</span></span><br><span class="line">    optimizer.step()</span><br><span class="line"></span><br><span class="line"> <span class="comment">#Output</span></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;w = &#x27;</span>, model.linear.weight.item())</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;b = &#x27;</span>, model.linear.bias.item())</span><br><span class="line"></span><br><span class="line"><span class="comment">#TestModel</span></span><br><span class="line">x_test = torch.Tensor([[<span class="number">4.0</span>]])</span><br><span class="line">y_test = model(x_test)</span><br><span class="line"></span><br><span class="line"><span class="built_in">print</span>(<span class="string">&#x27;y_pred = &#x27;</span>,y_test.data)</span><br></pre></td></tr></table></figure><p><img src="/"></p>]]></content>
      
      
      <categories>
          
          <category> python </category>
          
      </categories>
      
      
        <tags>
            
            <tag> python </tag>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
  
  
</search>
